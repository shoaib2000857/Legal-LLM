{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8bc8ea",
   "metadata": {},
   "source": [
    "# Complete Guide to Fine-tuning LLMs for Legal Applications\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **What is LLM Fine-tuning?** - Core concepts and terminology\n",
    "2. **Legal Data Characteristics** - Understanding domain-specific challenges\n",
    "3. **Data Preparation** - Formatting data for instruction-following models\n",
    "4. **Model Selection** - Choosing the right base model for legal tasks\n",
    "5. **Training Process** - Step-by-step fine-tuning with practical code\n",
    "6. **Evaluation** - Measuring model performance objectively\n",
    "7. **Deployment** - Using your fine-tuned model for real applications\n",
    "\n",
    "## üöÄ What You'll Build\n",
    "\n",
    "We'll fine-tune Google's FLAN-T5 model on Indian Constitutional law Q&A data to create a specialized legal assistant that can:\n",
    "- Answer questions about Indian constitutional provisions\n",
    "- Understand legal terminology and context\n",
    "- Provide accurate, relevant responses to legal queries\n",
    "\n",
    "Let's start this exciting journey into legal AI! üßë‚Äç‚öñÔ∏èü§ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e7b2bf",
   "metadata": {},
   "source": [
    "## 1. Understanding LLM Fine-tuning\n",
    "\n",
    "### What is Fine-tuning?\n",
    "\n",
    "**Fine-tuning** is like giving a smart student (pre-trained model) additional specialized training in a specific subject (legal domain).\n",
    "\n",
    "```\n",
    "Pre-trained Model ‚Üí Legal Data Training ‚Üí Specialized Legal Model\n",
    "     (General)           (Domain)              (Expert)\n",
    "```\n",
    "\n",
    "### Why Fine-tune for Legal Applications?\n",
    "\n",
    "1. **Specialized Vocabulary**: Legal text contains unique terminology (jurisprudence, habeas corpus, etc.)\n",
    "2. **Complex Reasoning**: Legal questions often require understanding precedents and context\n",
    "3. **Precision Requirements**: Legal applications demand high accuracy\n",
    "4. **Domain Knowledge**: Understanding constitutional principles, legal procedures\n",
    "\n",
    "### Types of Fine-tuning\n",
    "\n",
    "- **Full Fine-tuning**: Update all model parameters (computationally expensive)\n",
    "- **Parameter-Efficient**: Update only a subset of parameters (LoRA, AdaLoRA)\n",
    "- **Instruction Tuning**: Teaching models to follow specific instruction formats\n",
    "\n",
    "For this tutorial, we'll use **instruction tuning** which is perfect for Q&A tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a798df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing all necessary libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(\"üéØ Ready to explore legal LLM fine-tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306bb5a",
   "metadata": {},
   "source": [
    "## 2. Exploring Our Legal Dataset\n",
    "\n",
    "Before we can fine-tune a model, we need to understand our data. Let's explore the Indian Constitutional Q&A dataset.\n",
    "\n",
    "### Key Questions to Answer:\n",
    "- How many Q&A pairs do we have?\n",
    "- What's the typical length of questions and answers?\n",
    "- What legal terms appear most frequently?\n",
    "- Are there patterns in how questions are structured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the constitutional Q&A dataset\n",
    "print(\"üìñ Loading Indian Constitutional Q&A Dataset...\")\n",
    "\n",
    "with open('constitution_qa.json', 'r', encoding='utf-8') as f:\n",
    "    legal_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"üìä Total Q&A pairs: {len(legal_data)}\")\n",
    "\n",
    "# Display first few examples to understand the structure\n",
    "print(\"\\nüîç Sample Q&A pairs:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, item in enumerate(legal_data[:3]):\n",
    "    print(f\"\\nüìù Example {i+1}:\")\n",
    "    print(f\"‚ùì Question: {item['question']}\")\n",
    "    print(f\"üí° Answer: {item['answer']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(legal_data)\n",
    "\n",
    "print(\"üìä Dataset Structure Analysis\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Analyze text characteristics\n",
    "df['question_length_chars'] = df['question'].str.len()\n",
    "df['answer_length_chars'] = df['answer'].str.len()\n",
    "df['question_words'] = df['question'].str.split().str.len()\n",
    "df['answer_words'] = df['answer'].str.split().str.len()\n",
    "\n",
    "print(\"\\nüìè Text Length Statistics:\")\n",
    "stats_df = df[['question_length_chars', 'answer_length_chars', 'question_words', 'answer_words']].describe()\n",
    "print(stats_df)\n",
    "\n",
    "# Check for any data quality issues\n",
    "print(f\"\\nüîç Data Quality Check:\")\n",
    "print(f\"Questions with missing data: {df['question'].isna().sum()}\")\n",
    "print(f\"Answers with missing data: {df['answer'].isna().sum()}\")\n",
    "print(f\"Empty questions: {(df['question'].str.strip() == '').sum()}\")\n",
    "print(f\"Empty answers: {(df['answer'].str.strip() == '').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b726e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('üìä Legal Dataset Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Question length distribution\n",
    "axes[0, 0].hist(df['question_words'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('üìù Question Length Distribution', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Number of words')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(df['question_words'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {df[\"question_words\"].mean():.1f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Answer length distribution\n",
    "axes[0, 1].hist(df['answer_words'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('üí° Answer Length Distribution', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Number of words')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].axvline(df['answer_words'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {df[\"answer_words\"].mean():.1f}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Question vs Answer length relationship\n",
    "scatter = axes[0, 2].scatter(df['question_words'], df['answer_words'], alpha=0.6, c='purple')\n",
    "axes[0, 2].set_title('üîó Question vs Answer Length', fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Question length (words)')\n",
    "axes[0, 2].set_ylabel('Answer length (words)')\n",
    "\n",
    "# Add correlation coefficient\n",
    "correlation = df['question_words'].corr(df['answer_words'])\n",
    "axes[0, 2].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "                transform=axes[0, 2].transAxes, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\"))\n",
    "\n",
    "# 4. Box plot comparison\n",
    "data_for_box = [df['question_words'], df['answer_words']]\n",
    "bp = axes[1, 0].boxplot(data_for_box, labels=['Questions', 'Answers'], patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('skyblue')\n",
    "bp['boxes'][1].set_facecolor('lightgreen')\n",
    "axes[1, 0].set_title('üì¶ Length Distribution Comparison', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Number of words')\n",
    "\n",
    "# 5. Cumulative distribution\n",
    "sorted_q = np.sort(df['question_words'])\n",
    "sorted_a = np.sort(df['answer_words'])\n",
    "y_q = np.arange(1, len(sorted_q) + 1) / len(sorted_q)\n",
    "y_a = np.arange(1, len(sorted_a) + 1) / len(sorted_a)\n",
    "\n",
    "axes[1, 1].plot(sorted_q, y_q, label='Questions', linewidth=2)\n",
    "axes[1, 1].plot(sorted_a, y_a, label='Answers', linewidth=2)\n",
    "axes[1, 1].set_title('üìà Cumulative Distribution', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Number of words')\n",
    "axes[1, 1].set_ylabel('Cumulative probability')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Data summary pie chart\n",
    "lengths = {\n",
    "    'Short Questions (<10 words)': (df['question_words'] < 10).sum(),\n",
    "    'Medium Questions (10-25 words)': ((df['question_words'] >= 10) & (df['question_words'] <= 25)).sum(),\n",
    "    'Long Questions (>25 words)': (df['question_words'] > 25).sum()\n",
    "}\n",
    "\n",
    "axes[1, 2].pie(lengths.values(), labels=lengths.keys(), autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 2].set_title('üìä Question Length Categories', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Key Insights:\")\n",
    "print(f\"‚Ä¢ Average question length: {df['question_words'].mean():.1f} words\")\n",
    "print(f\"‚Ä¢ Average answer length: {df['answer_words'].mean():.1f} words\")\n",
    "print(f\"‚Ä¢ Most questions are between {df['question_words'].quantile(0.25):.0f} and {df['question_words'].quantile(0.75):.0f} words\")\n",
    "print(f\"‚Ä¢ Correlation between question and answer length: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze legal terminology and patterns\n",
    "print(\"‚öñÔ∏è Legal Terminology Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Combine all text for analysis\n",
    "all_questions = ' '.join(df['question'].tolist())\n",
    "all_answers = ' '.join(df['answer'].tolist())\n",
    "\n",
    "# Define legal terms pattern - more comprehensive\n",
    "legal_terms_pattern = r'\\b(?:Parliament|Constitution|State|Union|Bill|Act|Article|Schedule|Amendment|Court|Justice|Law|Right|Duty|Citizen|Government|Territory|President|Governor|Minister|Legislature|Judiciary|Executive|Council|Assembly|Election|Democracy|Republic|Federal|Fundamental|Directive|Emergency|Ordinance|Writ|Petition|Appeal|Jurisdiction|Sovereignty|Secularism|Socialism)\\b'\n",
    "\n",
    "# Extract legal terms\n",
    "legal_terms_questions = re.findall(legal_terms_pattern, all_questions, re.IGNORECASE)\n",
    "legal_terms_answers = re.findall(legal_terms_pattern, all_answers, re.IGNORECASE)\n",
    "\n",
    "print(\"üîç Most Common Legal Terms in Questions:\")\n",
    "question_counter = Counter([term.title() for term in legal_terms_questions])\n",
    "for i, (term, count) in enumerate(question_counter.most_common(15), 1):\n",
    "    print(f\"  {i:2d}. {term:<15} : {count:4d} occurrences\")\n",
    "\n",
    "print(\"\\nüí° Most Common Legal Terms in Answers:\")\n",
    "answer_counter = Counter([term.title() for term in legal_terms_answers])\n",
    "for i, (term, count) in enumerate(answer_counter.most_common(15), 1):\n",
    "    print(f\"  {i:2d}. {term:<15} : {count:4d} occurrences\")\n",
    "\n",
    "# Analyze question patterns\n",
    "print(\"\\n‚ùì Question Pattern Analysis:\")\n",
    "question_starters = []\n",
    "for question in df['question']:\n",
    "    words = question.split()\n",
    "    if len(words) >= 3:\n",
    "        starter = ' '.join(words[:3]).lower()\n",
    "        question_starters.append(starter)\n",
    "\n",
    "pattern_counter = Counter(question_starters)\n",
    "print(\"\\nüîÑ Most Common Question Patterns (first 3 words):\")\n",
    "for i, (pattern, count) in enumerate(pattern_counter.most_common(12), 1):\n",
    "    print(f\"  {i:2d}. '{pattern.title():<20}' : {count:3d} times\")\n",
    "\n",
    "# Visualize top legal terms\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top terms in questions\n",
    "top_q_terms = dict(question_counter.most_common(10))\n",
    "ax1.barh(list(top_q_terms.keys()), list(top_q_terms.values()), color='skyblue')\n",
    "ax1.set_title('üîç Top Legal Terms in Questions', fontweight='bold')\n",
    "ax1.set_xlabel('Frequency')\n",
    "\n",
    "# Top terms in answers\n",
    "top_a_terms = dict(answer_counter.most_common(10))\n",
    "ax2.barh(list(top_a_terms.keys()), list(top_a_terms.values()), color='lightgreen')\n",
    "ax2.set_title('üí° Top Legal Terms in Answers', fontweight='bold')\n",
    "ax2.set_xlabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b50e1",
   "metadata": {},
   "source": [
    "## 3. Understanding the Fine-tuning Process\n",
    "\n",
    "### Step-by-Step Breakdown\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Raw Legal Data] --> B[Data Preprocessing]\n",
    "    B --> C[Instruction Formatting]\n",
    "    C --> D[Train/Val/Test Split]\n",
    "    D --> E[Tokenization]\n",
    "    E --> F[Model Loading]\n",
    "    F --> G[Training Loop]\n",
    "    G --> H[Validation]\n",
    "    H --> I[Model Evaluation]\n",
    "    I --> J[Deployment]\n",
    "```\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Instruction Formatting**: Converting Q&A pairs into instruction-following format\n",
    "2. **Tokenization**: Converting text to numbers the model understands\n",
    "3. **Training Loop**: Iteratively updating model weights based on legal data\n",
    "4. **Validation**: Monitoring performance to prevent overfitting\n",
    "5. **Evaluation**: Measuring how well the model performs on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Format data for instruction-following\n",
    "print(\"üîß Data Formatting for Instruction-Following\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def format_legal_instruction(question, answer, task_type=\"constitutional_qa\"):\n",
    "    \"\"\"\n",
    "    Format a legal Q&A pair into instruction-following format for T5\n",
    "    \n",
    "    Args:\n",
    "        question (str): The legal question\n",
    "        answer (str): The corresponding answer\n",
    "        task_type (str): Type of legal task\n",
    "    \n",
    "    Returns:\n",
    "        dict: Formatted instruction with input and target text\n",
    "    \"\"\"\n",
    "    # Create an instruction that clearly defines the task\n",
    "    if task_type == \"constitutional_qa\":\n",
    "        instruction = f\"Answer the following question about Indian constitutional law: {question}\"\n",
    "    else:\n",
    "        instruction = f\"Answer this legal question: {question}\"\n",
    "    \n",
    "    return {\n",
    "        'input_text': instruction,\n",
    "        'target_text': answer,\n",
    "        'task_type': task_type,\n",
    "        'original_question': question,\n",
    "        'original_answer': answer\n",
    "    }\n",
    "\n",
    "# Apply formatting to our dataset\n",
    "print(\"üîÑ Formatting legal Q&A pairs...\")\n",
    "formatted_legal_data = []\n",
    "\n",
    "for i, item in enumerate(legal_data):\n",
    "    formatted_item = format_legal_instruction(\n",
    "        item['question'], \n",
    "        item['answer'], \n",
    "        \"constitutional_qa\"\n",
    "    )\n",
    "    formatted_legal_data.append(formatted_item)\n",
    "\n",
    "print(f\"‚úÖ Formatted {len(formatted_legal_data)} legal Q&A pairs\")\n",
    "\n",
    "# Show examples of formatted data\n",
    "print(\"\\nüìù Examples of Formatted Instructions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, item in enumerate(formatted_legal_data[:3]):\n",
    "    print(f\"\\nüîç Example {i+1}:\")\n",
    "    print(f\"üì• INPUT: {item['input_text']}\")\n",
    "    print(f\"üì§ TARGET: {item['target_text']}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Analyze input/target lengths after formatting\n",
    "input_lengths = [len(item['input_text'].split()) for item in formatted_legal_data]\n",
    "target_lengths = [len(item['target_text'].split()) for item in formatted_legal_data]\n",
    "\n",
    "print(f\"\\nüìä Formatted Data Statistics:\")\n",
    "print(f\"Average input length: {np.mean(input_lengths):.1f} words\")\n",
    "print(f\"Average target length: {np.mean(target_lengths):.1f} words\")\n",
    "print(f\"Max input length: {max(input_lengths)} words\")\n",
    "print(f\"Max target length: {max(target_lengths)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d985852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split data into train/validation/test sets\n",
    "print(\"üìä Data Splitting Strategy\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split ratios\n",
    "train_ratio = 0.70  # 70% for training\n",
    "val_ratio = 0.15    # 15% for validation  \n",
    "test_ratio = 0.15   # 15% for final testing\n",
    "\n",
    "print(f\"üìà Split Strategy:\")\n",
    "print(f\"  ‚Ä¢ Training:   {train_ratio*100:.0f}% ({int(len(formatted_legal_data) * train_ratio):,} examples)\")\n",
    "print(f\"  ‚Ä¢ Validation: {val_ratio*100:.0f}% ({int(len(formatted_legal_data) * val_ratio):,} examples)\")\n",
    "print(f\"  ‚Ä¢ Testing:    {test_ratio*100:.0f}% ({int(len(formatted_legal_data) * test_ratio):,} examples)\")\n",
    "\n",
    "# First split: separate test set\n",
    "train_val_data, test_data = train_test_split(\n",
    "    formatted_legal_data, \n",
    "    test_size=test_ratio, \n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: separate validation from training\n",
    "train_data, val_data = train_test_split(\n",
    "    train_val_data, \n",
    "    test_size=val_ratio/(train_ratio + val_ratio),  # Adjust ratio for remaining data\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data Split Complete:\")\n",
    "print(f\"  ‚Ä¢ Training set:   {len(train_data):,} examples ({len(train_data)/len(formatted_legal_data)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Validation set: {len(val_data):,} examples ({len(val_data)/len(formatted_legal_data)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Test set:       {len(test_data):,} examples ({len(test_data)/len(formatted_legal_data)*100:.1f}%)\")\n",
    "\n",
    "# Visualize the split\n",
    "plt.figure(figsize=(10, 6))\n",
    "splits = ['Training', 'Validation', 'Test']\n",
    "sizes = [len(train_data), len(val_data), len(test_data)]\n",
    "colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "\n",
    "plt.pie(sizes, labels=splits, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "plt.title('üìä Dataset Split Distribution', fontsize=14, fontweight='bold')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Why This Split Strategy?\")\n",
    "print(f\"\"\"\n",
    "üèãÔ∏è Training Set (70%): Large enough to learn legal patterns and terminology\n",
    "üìè Validation Set (15%): Monitor overfitting and tune hyperparameters\n",
    "üß™ Test Set (15%): Unbiased evaluation of final model performance\n",
    "\n",
    "This ensures our legal LLM generalizes well to new constitutional questions!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d693237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Save prepared data for training\n",
    "print(\"üíæ Saving Prepared Legal Dataset\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Create data splits dictionary\n",
    "legal_data_splits = {\n",
    "    'train': train_data,\n",
    "    'validation': val_data,\n",
    "    'test': test_data,\n",
    "    'metadata': {\n",
    "        'total_examples': len(formatted_legal_data),\n",
    "        'train_size': len(train_data),\n",
    "        'val_size': len(val_data),\n",
    "        'test_size': len(test_data),\n",
    "        'task_type': 'constitutional_qa',\n",
    "        'format_version': '1.0',\n",
    "        'creation_date': '2025-08-16'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as pickle for easy loading during training\n",
    "with open('legal_data_splits.pkl', 'wb') as f:\n",
    "    pickle.dump(legal_data_splits, f)\n",
    "\n",
    "# Also save as JSON for portability\n",
    "with open('legal_data_splits.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(legal_data_splits, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ Data splits saved successfully!\")\n",
    "print(\"üìÅ Files created:\")\n",
    "print(\"  ‚Ä¢ legal_data_splits.pkl (Python pickle format)\")\n",
    "print(\"  ‚Ä¢ legal_data_splits.json (JSON format)\")\n",
    "\n",
    "# Display sample from each split\n",
    "print(f\"\\nüîç Sample from each split:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "splits = [('Training', train_data), ('Validation', val_data), ('Test', test_data)]\n",
    "for split_name, split_data in splits:\n",
    "    sample = split_data[0]\n",
    "    print(f\"\\nüìö {split_name} Sample:\")\n",
    "    print(f\"‚ùì Input: {sample['input_text'][:100]}...\")\n",
    "    print(f\"üí° Target: {sample['target_text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbcbee",
   "metadata": {},
   "source": [
    "## 4. Model Selection and Architecture\n",
    "\n",
    "### Why FLAN-T5 for Legal Applications?\n",
    "\n",
    "**FLAN-T5** (Fine-tuned LAnguage Net - Text-to-Text Transfer Transformer) is perfect for legal Q&A because:\n",
    "\n",
    "1. **Instruction-Tuned**: Pre-trained to follow instructions and answer questions\n",
    "2. **Text-to-Text**: Can handle various legal tasks (Q&A, summarization, classification)\n",
    "3. **Efficient**: Smaller models (like T5-small) work well for specialized domains\n",
    "4. **Proven**: Strong performance on reasoning tasks similar to legal analysis\n",
    "\n",
    "### Model Sizes Available:\n",
    "- **T5-small**: 60M parameters (good for learning/prototyping)\n",
    "- **T5-base**: 220M parameters (balanced performance/efficiency)\n",
    "- **T5-large**: 770M parameters (higher quality, more resources needed)\n",
    "\n",
    "For this tutorial, we'll use **FLAN-T5-small** as it's:\n",
    "- Fast to train ‚ö°\n",
    "- Runs on modest hardware üíª\n",
    "- Good for learning concepts üìö\n",
    "- Adequate for specialized legal domain üèõÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for fine-tuning\n",
    "print(\"üì¶ Installing Required Libraries for LLM Fine-tuning\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Note: Run these installations in your environment\n",
    "required_packages = [\n",
    "    \"transformers>=4.30.0\",\n",
    "    \"datasets>=2.12.0\", \n",
    "    \"torch>=2.0.0\",\n",
    "    \"accelerate>=0.20.0\",\n",
    "    \"evaluate>=0.4.0\",\n",
    "    \"rouge-score>=0.1.2\",\n",
    "    \"sentencepiece>=0.1.99\"\n",
    "]\n",
    "\n",
    "print(\"üìã Required packages for legal LLM fine-tuning:\")\n",
    "for i, package in enumerate(required_packages, 1):\n",
    "    print(f\"  {i}. {package}\")\n",
    "\n",
    "print(f\"\\nüí° Installation command:\")\n",
    "print(f\"pip install {' '.join(required_packages)}\")\n",
    "\n",
    "print(f\"\\nüöÄ Once installed, we can proceed with:\")\n",
    "print(f\"  ‚úÖ Loading pre-trained FLAN-T5 model\")\n",
    "print(f\"  ‚úÖ Setting up tokenizer for legal text\")\n",
    "print(f\"  ‚úÖ Configuring training parameters\")\n",
    "print(f\"  ‚úÖ Starting the fine-tuning process\")\n",
    "\n",
    "# Check if transformers is available\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"\\n‚úÖ Transformers library available: v{transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(f\"\\n‚ùå Transformers library not found. Please install it first.\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch available: v{torch.__version__}\")\n",
    "    print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    print(f\"‚ùå PyTorch not found. Please install it first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9879c2",
   "metadata": {},
   "source": [
    "## 5. Complete Fine-tuning Implementation\n",
    "\n",
    "The next section contains the complete code for fine-tuning FLAN-T5 on legal data. This includes:\n",
    "\n",
    "1. **Model & Tokenizer Loading** ü§ñ\n",
    "2. **Data Tokenization** üî§  \n",
    "3. **Training Configuration** ‚öôÔ∏è\n",
    "4. **Training Loop** üîÑ\n",
    "5. **Evaluation** üìä\n",
    "6. **Model Saving** üíæ\n",
    "\n",
    "### Key Training Parameters:\n",
    "\n",
    "- **Learning Rate**: `5e-5` (conservative for legal precision)\n",
    "- **Batch Size**: `8` (adjust based on your GPU memory)\n",
    "- **Epochs**: `3` (prevent overfitting on specialized data)\n",
    "- **Max Length**: `512` tokens for input, `128` for output\n",
    "- **Evaluation**: ROUGE scores for text quality assessment\n",
    "\n",
    "Ready to train your legal AI assistant? üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a68c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Legal LLM Fine-tuning Implementation\n",
    "print(\"üöÄ Legal LLM Fine-tuning - Complete Implementation\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Uncomment and run this cell after installing required packages\n",
    "\"\"\"\n",
    "# Import required libraries for fine-tuning\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration for legal LLM fine-tuning\n",
    "class LegalLLMConfig:\n",
    "    # Model configuration\n",
    "    model_name = \"google/flan-t5-small\"\n",
    "    \n",
    "    # Data configuration\n",
    "    max_input_length = 512\n",
    "    max_target_length = 128\n",
    "    \n",
    "    # Training configuration\n",
    "    output_dir = \"./models/legal_constitutional_qa\"\n",
    "    num_epochs = 3\n",
    "    batch_size = 8\n",
    "    learning_rate = 5e-5\n",
    "    warmup_ratio = 0.1\n",
    "    weight_decay = 0.01\n",
    "    \n",
    "    # Evaluation configuration\n",
    "    eval_strategy = \"steps\"\n",
    "    eval_steps = 500\n",
    "    logging_steps = 100\n",
    "    save_steps = 500\n",
    "    \n",
    "    # Hardware configuration\n",
    "    fp16 = torch.cuda.is_available()  # Use mixed precision if GPU available\n",
    "    dataloader_num_workers = 4\n",
    "\n",
    "config = LegalLLMConfig()\n",
    "\n",
    "print(f\"üéØ Training Configuration:\")\n",
    "print(f\"  ‚Ä¢ Model: {config.model_name}\")\n",
    "print(f\"  ‚Ä¢ Max input length: {config.max_input_length} tokens\")\n",
    "print(f\"  ‚Ä¢ Max target length: {config.max_target_length} tokens\")\n",
    "print(f\"  ‚Ä¢ Batch size: {config.batch_size}\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {config.learning_rate}\")\n",
    "print(f\"  ‚Ä¢ Number of epochs: {config.num_epochs}\")\n",
    "print(f\"  ‚Ä¢ Output directory: {config.output_dir}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "print(f\"‚úÖ Output directory created: {config.output_dir}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìù Note: Uncomment the code above after installing required packages!\")\n",
    "print(\"This implementation provides a complete pipeline for fine-tuning on legal data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4de148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization Function for Legal Data\n",
    "print(\"üî§ Tokenization Strategy for Legal Text\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# This shows how we'll tokenize the legal data\n",
    "tokenization_example = \"\"\"\n",
    "def tokenize_legal_data(examples, tokenizer, config):\n",
    "    '''\n",
    "    Tokenize legal Q&A data for instruction-following format\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of legal Q&A examples\n",
    "        tokenizer: FLAN-T5 tokenizer\n",
    "        config: Training configuration\n",
    "    \n",
    "    Returns:\n",
    "        Tokenized inputs and labels\n",
    "    '''\n",
    "    # Tokenize inputs (questions with instruction format)\n",
    "    model_inputs = tokenizer(\n",
    "        examples['input_text'],\n",
    "        max_length=config.max_input_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets (answers)\n",
    "    labels = tokenizer(\n",
    "        examples['target_text'],\n",
    "        max_length=config.max_target_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Replace padding token id's of the labels by -100 \n",
    "    # so they are ignored in the loss computation\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Key Tokenization Concepts:\")\n",
    "print(\"=\" * 35)\n",
    "print(\"1. üì• Input Tokenization:\")\n",
    "print(\"   ‚Ä¢ Converts legal questions to token IDs\")\n",
    "print(\"   ‚Ä¢ Handles special tokens (instruction format)\")\n",
    "print(\"   ‚Ä¢ Truncates/pads to fixed length\")\n",
    "\n",
    "print(\"\\n2. üì§ Target Tokenization:\")\n",
    "print(\"   ‚Ä¢ Converts legal answers to token IDs\") \n",
    "print(\"   ‚Ä¢ Padding tokens replaced with -100 (ignored in loss)\")\n",
    "print(\"   ‚Ä¢ Ensures consistent sequence lengths\")\n",
    "\n",
    "print(\"\\n3. üéØ Why This Matters for Legal AI:\")\n",
    "print(\"   ‚Ä¢ Legal text often contains long, complex sentences\")\n",
    "print(\"   ‚Ä¢ Proper tokenization preserves legal terminology\")\n",
    "print(\"   ‚Ä¢ Instruction format teaches the model legal Q&A structure\")\n",
    "\n",
    "# Show example of what tokenization looks like\n",
    "sample_question = \"What is India according to the Union and its Territory?\"\n",
    "sample_answer = \"India, that is Bharat, shall be a Union of States.\"\n",
    "sample_instruction = f\"Answer the following question about Indian constitutional law: {sample_question}\"\n",
    "\n",
    "print(f\"\\nüìù Tokenization Example:\")\n",
    "print(f\"üî§ Raw Input: '{sample_instruction}'\")\n",
    "print(f\"üî§ Raw Target: '{sample_answer}'\")\n",
    "print(f\"‚ö° After tokenization: Numbers that the model understands!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dbd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop and Evaluation Metrics\n",
    "print(\"üèãÔ∏è Training Process and Evaluation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "training_process = \"\"\"\n",
    "Training a Legal LLM involves several key steps:\n",
    "\n",
    "1. üîÑ FORWARD PASS\n",
    "   ‚Ä¢ Model processes legal question\n",
    "   ‚Ä¢ Generates answer prediction\n",
    "   ‚Ä¢ Compares with correct legal answer\n",
    "\n",
    "2. üìä LOSS CALCULATION  \n",
    "   ‚Ä¢ CrossEntropy loss measures prediction errors\n",
    "   ‚Ä¢ Higher loss = model is more confused\n",
    "   ‚Ä¢ Goal: Minimize loss on legal Q&A pairs\n",
    "\n",
    "3. ‚¨ÖÔ∏è BACKWARD PASS\n",
    "   ‚Ä¢ Calculate gradients (how to improve)\n",
    "   ‚Ä¢ Update model weights\n",
    "   ‚Ä¢ Model gets better at legal reasoning\n",
    "\n",
    "4. üìà VALIDATION\n",
    "   ‚Ä¢ Test on unseen legal questions\n",
    "   ‚Ä¢ Monitor for overfitting\n",
    "   ‚Ä¢ Early stopping if performance degrades\n",
    "\n",
    "5. üíæ CHECKPOINTING\n",
    "   ‚Ä¢ Save best model weights\n",
    "   ‚Ä¢ Resume training if interrupted\n",
    "   ‚Ä¢ Keep track of training progress\n",
    "\"\"\"\n",
    "\n",
    "print(training_process)\n",
    "\n",
    "evaluation_metrics = \"\"\"\n",
    "üìä EVALUATION METRICS FOR LEGAL LLM\n",
    "\n",
    "1. üéØ ROUGE Scores (Text Quality)\n",
    "   ‚Ä¢ ROUGE-1: Word overlap between prediction and reference\n",
    "   ‚Ä¢ ROUGE-2: Bigram overlap (phrase matching)\n",
    "   ‚Ä¢ ROUGE-L: Longest common subsequence\n",
    "   \n",
    "2. ‚öñÔ∏è Legal-Specific Metrics\n",
    "   ‚Ä¢ Constitutional Accuracy: Correct constitutional references\n",
    "   ‚Ä¢ Legal Term Precision: Proper use of legal terminology\n",
    "   ‚Ä¢ Factual Consistency: Accurate legal facts and precedents\n",
    "\n",
    "3. üîç Qualitative Assessment\n",
    "   ‚Ä¢ Human evaluation by legal experts\n",
    "   ‚Ä¢ Logical reasoning quality\n",
    "   ‚Ä¢ Appropriate legal citations\n",
    "\"\"\"\n",
    "\n",
    "print(evaluation_metrics)\n",
    "\n",
    "# Sample evaluation code structure\n",
    "print(\"üíª Evaluation Implementation Preview:\")\n",
    "print(\"=\" * 40)\n",
    "eval_code = '''\n",
    "def evaluate_legal_model(model, eval_dataset, tokenizer):\n",
    "    \"\"\"Evaluate fine-tuned legal model\"\"\"\n",
    "    \n",
    "    # Load evaluation metrics\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for example in eval_dataset:\n",
    "        # Generate prediction\n",
    "        inputs = tokenizer(example['input_text'], return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs, max_length=128)\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        references.append(example['target_text'])\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    rouge_scores = rouge.compute(\n",
    "        predictions=predictions, \n",
    "        references=references\n",
    "    )\n",
    "    \n",
    "    return rouge_scores, predictions, references\n",
    "'''\n",
    "\n",
    "print(eval_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e6ae9",
   "metadata": {},
   "source": [
    "## 6. Next Steps: Running Your Legal LLM Training\n",
    "\n",
    "### üöÄ Ready to Train? Here's Your Action Plan:\n",
    "\n",
    "#### Step 1: Environment Setup\n",
    "```bash\n",
    "# Install required packages\n",
    "pip install transformers datasets torch accelerate evaluate rouge-score sentencepiece\n",
    "\n",
    "# Verify GPU availability (optional but recommended)\n",
    "python -c \"import torch; print(f'CUDA: {torch.cuda.is_available()}')\"\n",
    "```\n",
    "\n",
    "#### Step 2: Run the Training\n",
    "1. Uncomment the training code in the cells above\n",
    "2. Execute each cell in sequence\n",
    "3. Monitor training progress in the output logs\n",
    "4. Training will take approximately 1-3 hours on GPU\n",
    "\n",
    "#### Step 3: Test Your Model\n",
    "```python\n",
    "# Load your trained model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/legal_constitutional_qa\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./models/legal_constitutional_qa\")\n",
    "\n",
    "# Test with a legal question\n",
    "question = \"What are the fundamental rights guaranteed by the Indian Constitution?\"\n",
    "input_text = f\"Answer the following question about Indian constitutional law: {question}\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=128, do_sample=True, temperature=0.7)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"AI Answer: {answer}\")\n",
    "```\n",
    "\n",
    "### üéØ Expected Outcomes:\n",
    "- ‚úÖ A fine-tuned model specialized in Indian constitutional law\n",
    "- ‚úÖ Ability to answer complex legal questions accurately  \n",
    "- ‚úÖ Understanding of legal terminology and concepts\n",
    "- ‚úÖ Ready-to-deploy legal AI assistant\n",
    "\n",
    "### üîÑ Iterative Improvement:\n",
    "1. **Collect Feedback**: Test with legal experts\n",
    "2. **Data Augmentation**: Add more diverse legal Q&A pairs\n",
    "3. **Hyperparameter Tuning**: Optimize learning rate, batch size\n",
    "4. **Advanced Techniques**: Try LoRA, QLoRA for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324080d",
   "metadata": {},
   "source": [
    "## 7. Understanding What Happens During Training\n",
    "\n",
    "### üß† The Learning Process\n",
    "\n",
    "During fine-tuning, your legal LLM goes through several stages:\n",
    "\n",
    "#### Phase 1: Initial Adaptation (Epoch 1)\n",
    "- **What happens**: Model adjusts from general knowledge to legal domain\n",
    "- **Loss behavior**: High initially, drops rapidly  \n",
    "- **Learning focus**: Basic legal terminology and question patterns\n",
    "\n",
    "#### Phase 2: Legal Specialization (Epoch 2)\n",
    "- **What happens**: Model learns constitutional law specifics\n",
    "- **Loss behavior**: Steady decrease, more gradual\n",
    "- **Learning focus**: Complex legal relationships and reasoning\n",
    "\n",
    "#### Phase 3: Fine-tuning (Epoch 3)\n",
    "- **What happens**: Model polishes responses, reduces errors\n",
    "- **Loss behavior**: Small improvements, convergence\n",
    "- **Learning focus**: Precise legal language and edge cases\n",
    "\n",
    "### üìä Monitoring Your Training\n",
    "\n",
    "```\n",
    "Epoch 1/3: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% \n",
    "‚îú‚îÄ‚îÄ Train Loss: 2.341 ‚Üí 1.523\n",
    "‚îú‚îÄ‚îÄ Eval Loss: 1.876\n",
    "‚îú‚îÄ‚îÄ ROUGE-1: 0.342\n",
    "‚îî‚îÄ‚îÄ Learning Rate: 5e-05\n",
    "\n",
    "Epoch 2/3: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100%\n",
    "‚îú‚îÄ‚îÄ Train Loss: 1.523 ‚Üí 1.187  \n",
    "‚îú‚îÄ‚îÄ Eval Loss: 1.634\n",
    "‚îú‚îÄ‚îÄ ROUGE-1: 0.427\n",
    "‚îî‚îÄ‚îÄ Learning Rate: 3.5e-05\n",
    "\n",
    "Epoch 3/3: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100%\n",
    "‚îú‚îÄ‚îÄ Train Loss: 1.187 ‚Üí 1.089\n",
    "‚îú‚îÄ‚îÄ Eval Loss: 1.598\n",
    "‚îú‚îÄ‚îÄ ROUGE-1: 0.456\n",
    "‚îî‚îÄ‚îÄ Training Complete! üéâ\n",
    "```\n",
    "\n",
    "### üéØ Success Indicators:\n",
    "- ‚úÖ **Decreasing Loss**: Model is learning legal patterns\n",
    "- ‚úÖ **Stable Validation**: No overfitting on legal data\n",
    "- ‚úÖ **Improving ROUGE**: Better text quality and relevance\n",
    "- ‚úÖ **Legal Coherence**: Answers make sense legally\n",
    "\n",
    "Congratulations! You now understand the complete process of fine-tuning LLMs for legal applications! üéì‚öñÔ∏èü§ñ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
