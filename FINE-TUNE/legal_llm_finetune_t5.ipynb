{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09398ab9",
   "metadata": {},
   "source": [
    "\n",
    "# Fine-tune FLAN-T5-small for Indian Legal QA / Summarization\n",
    "\n",
    "This notebook walks you through a **minimal, reliable** fine-tuning pipeline on Indian legal text using a **small instruction-tuned model** (`google/flan-t5-small`).  \n",
    "It supports **two dataset modes**:\n",
    "1. **CSV** (e.g., Kaggle QA pairs) â€“ columns like `question`, `answer`, and optionally `context` or `text`  \n",
    "2. **Hugging Face Datasets** â€“ e.g., `viber1/indian-law-dataset` or similar\n",
    "\n",
    "> ðŸ”§ You can adapt this to any of your datasets by mapping columns in the **Column Mapping** cell.\n",
    "\n",
    "**What you'll do:**\n",
    "- Install libs\n",
    "- Load dataset (CSV or HF)\n",
    "- Clean & split\n",
    "- Tokenize to instruction format\n",
    "- Train with `Trainer`\n",
    "- Evaluate (ROUGE)\n",
    "- Save & run inference\n",
    "\n",
    "**Hardware**: Works on CPU, but GPU recommended (your RTX 4060 is perfect)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cecd10",
   "metadata": {},
   "source": [
    "## 0. Setup & Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1388669",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# from datasets import load_dataset, Dataset, DatasetDict\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Trainer, TrainingArguments\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# import evaluate\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# import numpy as np\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTorch:\u001b[39m\u001b[38;5;124m'\u001b[39m, torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA available:\u001b[39m\u001b[38;5;124m'\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\n",
    "# If running in a fresh environment, uncomment and run:\n",
    "# !pip install -U pip\n",
    "# !pip install \"torch>=2.2\" --index-url https://download.pytorch.org/whl/cu121  # for CUDA 12.x systems\n",
    "# !pip install transformers datasets accelerate evaluate sentencepiece rouge-score\n",
    "# #Optional: for faster training on NVIDIA GPUs (8-bit/4-bit quantization not needed for flan-t5-small)\n",
    "# !pip install bitsandbytes\n",
    "import os, sys, math, random\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print('Torch:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA device:', torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe719027",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e23c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Configuration ====\n",
    "# Choose data source: \"csv\" or \"huggingface\"\n",
    "DATA_SOURCE = \"csv\"  # \"csv\" | \"huggingface\"\n",
    "\n",
    "# If CSV: set your local CSV path here (Kaggle or custom)\n",
    "CSV_PATH = \"/path/to/your/indian_legal_qa.csv\"   # e.g., ./datasets/kaggle/qa.csv\n",
    "\n",
    "# If HuggingFace: set dataset name and splits/fields accordingly\n",
    "HF_DATASET_NAME = \"viber1/indian-law-dataset\"    # example; change to your dataset\n",
    "HF_SPLIT_TRAIN = \"train\"                          # update if dataset provides different splits\n",
    "HF_SPLIT_VALID = \"validation\"                      # fallback/auto-split used if not present\n",
    "\n",
    "# Task style: \"qa\" (question -> answer) or \"summarization\" (document -> summary)\n",
    "TASK_STYLE = \"qa\"  # \"qa\" | \"summarization\"\n",
    "\n",
    "# Model choice: use a small instruction-tuned seq2seq model\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "\n",
    "# Sequence lengths\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_RATIO = 0.03\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOGGING_STEPS = 50\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"./models/flan_t5_small_legal_qa\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae307ef",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a587d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data_from_csv(csv_path: str) -> Dataset:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Basic cleanup: drop fully empty rows\n",
    "    df = df.dropna(how='all')\n",
    "    # Add an 'id' if not present\n",
    "    if 'id' not in df.columns:\n",
    "        df.insert(0, 'id', range(1, len(df) + 1))\n",
    "    return Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "def try_load_hf_dataset(name: str, split_train: str, split_valid: str) -> DatasetDict:\n",
    "    try:\n",
    "        dsd = load_dataset(name)\n",
    "        # If the dataset already has train/validation/test, we use them directly\n",
    "        if isinstance(dsd, DatasetDict):\n",
    "            if \"train\" in dsd and \"validation\" in dsd:\n",
    "                return dsd\n",
    "            if \"train\" in dsd and \"test\" in dsd:\n",
    "                # create a validation split from test\n",
    "                dsd = DatasetDict({\n",
    "                    \"train\": dsd[\"train\"],\n",
    "                    \"validation\": dsd[\"test\"]\n",
    "                })\n",
    "                return dsd\n",
    "        # Or fallback to splits if provided\n",
    "        train = load_dataset(name, split=split_train)\n",
    "        valid = load_dataset(name, split=split_valid)\n",
    "        return DatasetDict({\"train\": train, \"validation\": valid})\n",
    "    except Exception as e:\n",
    "        print(\"HF dataset load failed:\", e)\n",
    "        raise\n",
    "\n",
    "if DATA_SOURCE == \"csv\":\n",
    "    ds_all = load_data_from_csv(CSV_PATH)\n",
    "    print(ds_all)\n",
    "    # Split 90/10 if no explicit validation set provided\n",
    "    ds_all = ds_all.train_test_split(test_size=0.1, seed=42)\n",
    "    ds = DatasetDict({\"train\": ds_all[\"train\"], \"validation\": ds_all[\"test\"]})\n",
    "else:\n",
    "    ds = try_load_hf_dataset(HF_DATASET_NAME, HF_SPLIT_TRAIN, HF_SPLIT_VALID)\n",
    "    print(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63425242",
   "metadata": {},
   "source": [
    "## 3. Column Mapping (adapt to your dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7627ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We try to infer common column names for question/answer/context/summary/text.\n",
    "# You can override manually below if needed.\n",
    "\n",
    "candidate_cols = {c.lower(): c for c in ds[\"train\"].column_names}\n",
    "print(\"Available columns:\", list(candidate_cols.values()))\n",
    "\n",
    "def pick_col(possible_names):\n",
    "    for name in possible_names:\n",
    "        if name in candidate_cols:\n",
    "            return candidate_cols[name]\n",
    "    return None\n",
    "\n",
    "# Default guesses (override if incorrect)\n",
    "QUESTION_COL = pick_col([\"question\", \"query\", \"prompt\", \"ques\"])\n",
    "ANSWER_COL   = pick_col([\"answer\", \"response\", \"target\", \"label\", \"output\"])\n",
    "CONTEXT_COL  = pick_col([\"context\", \"passage\", \"text\", \"document\", \"body\", \"content\"])\n",
    "SUMMARY_COL  = pick_col([\"summary\", \"abstract\", \"target\", \"label\"])\n",
    "\n",
    "print(\"Guessed mapping -> QUESTION:\", QUESTION_COL, \"| ANSWER:\", ANSWER_COL, \"| CONTEXT:\", CONTEXT_COL, \"| SUMMARY:\", SUMMARY_COL)\n",
    "\n",
    "# Manual override examples (uncomment and set if needed):\n",
    "# QUESTION_COL = \"your_question_column\"\n",
    "# ANSWER_COL   = \"your_answer_column\"\n",
    "# CONTEXT_COL  = \"your_context_or_text_column\"\n",
    "# SUMMARY_COL  = \"your_summary_column\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca10cd8",
   "metadata": {},
   "source": [
    "## 4. Basic Cleaning & Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4519cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def strip_or_none(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        return s if s else None\n",
    "    return x\n",
    "\n",
    "def clean_example(example):\n",
    "    ex = dict(example)\n",
    "    for k, v in ex.items():\n",
    "        if isinstance(v, str):\n",
    "            ex[k] = v.replace(\"\\u00a0\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \").strip()\n",
    "    return ex\n",
    "\n",
    "ds = ds.map(clean_example)\n",
    "\n",
    "print(\"Sample records from train split:\")\n",
    "for i in range(min(3, len(ds[\"train\"]))):\n",
    "    row = ds[\"train\"][i]\n",
    "    print({k: row.get(k) for k in [QUESTION_COL, ANSWER_COL, CONTEXT_COL, SUMMARY_COL] if k is not None})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b1ba54",
   "metadata": {},
   "source": [
    "## 5. Build Instruction-style Inputs & Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We convert data into instruction format for FLAN-T5:\n",
    "#   - QA:        input = f\"question: {Q}\\ncontext: {C}\"  -> target = A\n",
    "#   - Summarize: input = f\"summarize: {TEXT}\"            -> target = SUMMARY\n",
    "#\n",
    "# If context is missing for QA, we omit it.\n",
    "\n",
    "def build_qa_text(example):\n",
    "    q = example.get(QUESTION_COL) if QUESTION_COL else None\n",
    "    a = example.get(ANSWER_COL)   if ANSWER_COL   else None\n",
    "    c = example.get(CONTEXT_COL)  if CONTEXT_COL  else None\n",
    "\n",
    "    q = strip_or_none(q)\n",
    "    a = strip_or_none(a)\n",
    "    c = strip_or_none(c)\n",
    "\n",
    "    if q is None or a is None:\n",
    "        return {\"input_text\": None, \"target_text\": None}\n",
    "\n",
    "    if c:\n",
    "        inp = f\"question: {q}\\ncontext: {c}\"\n",
    "    else:\n",
    "        inp = f\"question: {q}\"\n",
    "    return {\"input_text\": inp, \"target_text\": a}\n",
    "\n",
    "def build_sum_text(example):\n",
    "    txt = example.get(CONTEXT_COL) or example.get(\"text\") or example.get(\"document\")\n",
    "    tgt = example.get(SUMMARY_COL)\n",
    "    txt = strip_or_none(txt)\n",
    "    tgt = strip_or_none(tgt)\n",
    "    if txt is None or tgt is None:\n",
    "        return {\"input_text\": None, \"target_text\": None}\n",
    "    return {\"input_text\": f\"summarize: {txt}\", \"target_text\": tgt}\n",
    "\n",
    "builder = build_qa_text if TASK_STYLE == \"qa\" else build_sum_text\n",
    "\n",
    "ds_fmt = ds.map(builder, remove_columns=ds[\"train\"].column_names)\n",
    "# Drop rows where either input or target is None\n",
    "ds_fmt = ds_fmt.filter(lambda x: x[\"input_text\"] is not None and x[\"target_text\"] is not None)\n",
    "\n",
    "print(ds_fmt)\n",
    "print(\"Example formatted record:\", ds_fmt[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5381f44b",
   "metadata": {},
   "source": [
    "## 6. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a3ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"target_text\"],\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            truncation=True\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = ds_fmt.map(tokenize_function, batched=True, remove_columns=ds_fmt[\"train\"].column_names)\n",
    "print(tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e9f594",
   "metadata": {},
   "source": [
    "## 7. Model & Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de817bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e88a1",
   "metadata": {},
   "source": [
    "## 8. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adda4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 2) for k, v in result.items()}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4eb050",
   "metadata": {},
   "source": [
    "## 9. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=LOGGING_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=LOGGING_STEPS,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    "    bf16=torch.cuda.is_available(),   # if Ampere+ GPU\n",
    "    fp16=not torch.cuda.is_available() and False,  # leave False if no GPU\n",
    "    gradient_accumulation_steps=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Training complete! Model saved to\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6dc138",
   "metadata": {},
   "source": [
    "## 10. Quick Evaluation / Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_answer(prompt, max_new_tokens=128, num_beams=4):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=num_beams\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Pick a sample from validation set\n",
    "sample = ds_fmt[\"validation\"][0]\n",
    "print(\"INPUT:\\n\", sample[\"input_text\"][:500])\n",
    "print(\"\\nREFERENCE:\\n\", sample[\"target_text\"][:500])\n",
    "print(\"\\nPREDICTION:\\n\", generate_answer(sample[\"input_text\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d7991",
   "metadata": {},
   "source": [
    "## 11. Inference Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69903830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reload from disk (optional: for a clean session)\n",
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(OUTPUT_DIR)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "\n",
    "def ask_legal(question: str, context: str = None) -> str:\n",
    "    if TASK_STYLE == \"qa\":\n",
    "        if context:\n",
    "            prompt = f\"question: {question}\\ncontext: {context}\"\n",
    "        else:\n",
    "            prompt = f\"question: {question}\"\n",
    "    else:\n",
    "        prompt = f\"summarize: {question if context is None else context}\"\n",
    "    return generate_answer(prompt)\n",
    "\n",
    "print(ask_legal(\"What is the punishment for theft under the IPC?\", \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9eae79",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Tips & Next Steps\n",
    "\n",
    "- **Column mapping:** If the guessed `QUESTION_COL`, `ANSWER_COL`, etc. are wrong, set them manually in the mapping cell.\n",
    "- **Long documents:** Increase `MAX_INPUT_LENGTH` (up to 1024 for small T5s), or chunk long contexts.\n",
    "- **Better models:** Upgrade to `google/flan-t5-base` or instruction-tune a 7B model with **QLoRA** (LoRA + 4-bit) once this pipeline works.\n",
    "- **Evaluation:** Use a held-out test set. Add metrics like BLEU, exact match, or task-specific scoring.\n",
    "- **Licensing:** Verify dataset licenses before training for redistribution/commercial use.\n",
    "- **Reproducibility:** Save `seed`, environment, and commit your config to git.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
